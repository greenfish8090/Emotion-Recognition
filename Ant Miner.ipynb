{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f3c9b2f",
   "metadata": {},
   "source": [
    "# An implementation of the Ant miner paper for Emotion Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87691e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c2f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Resources/Ravdess Dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7e8368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Level of discretization of the continuous data\n",
    "bins = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d77103",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pd.cut(df['f0'], bins=bins, labels=range(bins)), bins = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2923ed2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(pd.qcut(df['f0'], q=bins, labels=range(bins)), bins = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fed1fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "discreet = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9c10e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.drop('class', axis=1).columns:\n",
    "    #discreet[column] = pd.cut(df[column], bins=bins, labels=range(bins))\n",
    "    discreet[column] = pd.qcut(df[column], q=bins, duplicates='drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ee4d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_bins = discreet.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d38235",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for column in df.drop('class', axis=1).columns:\n",
    "    #discreet[column] = pd.cut(df[column], bins=bins, labels=range(bins))\n",
    "    discreet[column] = pd.qcut(df[column], q=bins, duplicates='drop', labels=range(temp_bins[column]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab65404",
   "metadata": {},
   "outputs": [],
   "source": [
    "discreet = discreet.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1facf74a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "discreet = discreet.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec3f0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "discreet = discreet.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bba82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "discreet['class'] = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e002973",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = discreet['class'].unique()\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6019853",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = np.split(discreet.sample(frac=1), [int(0.75*len(discreet))])\n",
    "train_len = len(train)\n",
    "test_len = len(test)\n",
    "total_len = train_len + test_len\n",
    "test_len, train_len, total_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d599cddb",
   "metadata": {},
   "source": [
    "# Entropy values\n",
    "H will be a matrix of size (bins x features). Computed in a vectorized way to save time (This would take like 10 mins if it was in a for loop, now it takes 10s). <br>\n",
    "Ok fine I'm looping through the 6 emotions so techhhhnically it's not fully vectorized. Anyway we're computing H only once so I took lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a53a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = np.zeros((bins, len(train.columns)-1, len(emotions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dff6d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most compute intensive step\n",
    "for e, emotion in enumerate(emotions):\n",
    "            H[:, :, e] = train.loc[train['class']==emotion].drop('class', axis=1).apply(pd.Series.value_counts).fillna(0).to_numpy()\n",
    "#H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a6bc2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sums = np.sum(H, axis=2)\n",
    "#sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd37cbe4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "repeated_sums = np.repeat(sums[:, :, np.newaxis], H.shape[2], axis=2)\n",
    "#repeated_sums[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7885e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.divide(H, repeated_sums, out=np.zeros_like(H), where=repeated_sums!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ee484",
   "metadata": {},
   "outputs": [],
   "source": [
    "logP = np.log2(P, out=np.zeros_like(P), where=P>0)\n",
    "#logP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f9d06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = -np.sum(np.multiply(P, logP), axis=2)\n",
    "#H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc7399f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.where(np.isnan(H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54108ab3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis = H[:, :-2]\n",
    "vis = vis.reshape(30,65)\n",
    "plt.imshow(vis)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62298a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = discreet.drop('class', axis=1).columns\n",
    "logK = np.log2(len(emotions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c7d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "min_coverage = 10\n",
    "max_uncovered = 10\n",
    "n_ants = 3000\n",
    "max_dups = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bf303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rule:\n",
    "    def __init__(self, train_set):\n",
    "        self.terms = {}\n",
    "        self.emotion = None\n",
    "        self.numerosity = 0\n",
    "        self.train_set = train_set\n",
    "        self.match_set = train_set\n",
    "    \n",
    "    def match(self, instance):\n",
    "        for key, value in instance.items():\n",
    "            if key in self.terms and self.terms[key] != value:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def new_match_set(self, term):\n",
    "        return self.match_set.loc[self.match_set['f' + str(term['feature'])] == term['class']]\n",
    "    \n",
    "    def unmatch_set(self):\n",
    "        return pd.concat([self.train_set, self.match_set, self.match_set]).drop_duplicates(keep=False)\n",
    "    \n",
    "    def correct_set(self):\n",
    "        return self.match_set.loc[self.match_set['class'] == self.emotion]\n",
    "    \n",
    "    def exists(self, feature):\n",
    "        return feature in self.terms\n",
    "            \n",
    "    def addTerm(self, term, new_matches):\n",
    "        self.terms[term['feature']] = term['class']\n",
    "        self.match_set = new_matches\n",
    "        self.numerosity += 1\n",
    "        \n",
    "    def quality(self):\n",
    "        matches = self.match_set\n",
    "        unmatches = self.unmatch_set()\n",
    "        TP = len(matches.loc[matches['class'] == self.emotion])\n",
    "        FP = len(matches.loc[matches['class'] != self.emotion])\n",
    "        FN = len(unmatches.loc[unmatches['class'] == self.emotion])\n",
    "        TN = len(unmatches.loc[unmatches['class'] != self.emotion])\n",
    "        \n",
    "        sens = TP / (TP + FN)\n",
    "        spec = TN / (FP + TN)\n",
    "        \n",
    "        return sens * spec\n",
    "        \n",
    "    def prune(self):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e88ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_probs(probs):\n",
    "    return probs / np.sum(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796acd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ant:\n",
    "    def __init__(self, index, train_set, n_tries, min_coverage):\n",
    "        self.index = index\n",
    "        self.rule = Rule(train_set)\n",
    "        self.n_tries = n_tries\n",
    "        self.min_coverage = min_coverage\n",
    "    \n",
    "    def traverse(self, T):\n",
    "        x = len(features) - self.rule.numerosity\n",
    "        Eta = logK - H\n",
    "        denom = x * np.sum(Eta, axis=0)\n",
    "        Eta = np.divide(Eta, denom)\n",
    "        \n",
    "        Probs = np.multiply(Eta, T)\n",
    "        denom = x * np.sum(Probs, axis=0)\n",
    "        Probs = np.divide(Probs, denom)\n",
    "        \n",
    "        should_continue = True\n",
    "        while(should_continue):\n",
    "            tries = self.n_tries\n",
    "            flattened = Probs.flatten()\n",
    "            while(tries):\n",
    "                chosen = int(np.random.choice(len(flattened), 1, p=flattened))\n",
    "                term = {'feature': chosen % len(features), 'class': chosen // len(features)}\n",
    "                new_cov = self.rule.new_match_set(term)\n",
    "                if len(new_cov) >= self.min_coverage:\n",
    "                    self.rule.addTerm(term, new_cov)\n",
    "                    print(\"Added \" + str(term))\n",
    "                    \n",
    "                    Probs[:, term['feature']] = 0\n",
    "                    Probs = normalize_probs(Probs)\n",
    "                    \n",
    "                    should_continue = True\n",
    "                    break\n",
    "                tries -= 1\n",
    "                should_continue = False\n",
    "                \n",
    "    def set_emotion(self):\n",
    "        matches = self.rule.match_set\n",
    "        self.rule.emotion = matches['class'].mode()[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a871bdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class World:\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_coverage = 10,\n",
    "        max_uncovered = 10,\n",
    "        n_ants = 500,\n",
    "        max_dups = 1,\n",
    "        chances_before_brutally_murdering_ant = 10,\n",
    "        train_set = train.copy(deep=True),\n",
    "    ):\n",
    "    \n",
    "        self.min_coverage = min_coverage\n",
    "        self.max_uncovered = max_uncovered\n",
    "        self.n_ants = n_ants\n",
    "        self.max_dups = max_dups\n",
    "        self.chances_before_brutally_murdering_ant = chances_before_brutally_murdering_ant\n",
    "        self.train_set = train_set\n",
    "        self.pheromone_map = np.ones_like(H) * (1 / (bins * len(features)))\n",
    "        self.discovered_rule_list = []\n",
    "\n",
    "    def deposit_pheromones(self, new_pheromones):\n",
    "        self.pheromone_map = self.pheromone_map + new_pheromones\n",
    "        \n",
    "    def evaporate_pheromones(self):\n",
    "        self.pheromone_map = self.pheromone_map / np.sum(self.pheromone_map)\n",
    "        \n",
    "    def plot_pheromones(self, a):\n",
    "        vis = self.pheromone_map[:, :-2]\n",
    "        vis = vis.reshape(30,65)\n",
    "        plt.imshow(vis)\n",
    "        plt.colorbar()\n",
    "        plt.savefig('Plots/Generation' + str(len(self.discovered_rule_list)) + 'ant' + str(a))\n",
    "        plt.clf()\n",
    "    \n",
    "    def dispatch_ants(self):\n",
    "        prev_rules = []\n",
    "        prev_qualities = []\n",
    "        dups = 0\n",
    "        for a in range(self.n_ants):\n",
    "            ant = Ant(a, self.train_set, self.chances_before_brutally_murdering_ant, self.min_coverage)\n",
    "            print(\"initialized ant \" + str(a))\n",
    "            ant.traverse(self.pheromone_map)\n",
    "            \n",
    "            for prev_rule in prev_rules:\n",
    "                if ant.rule == prev_rule:\n",
    "                    dups += 1\n",
    "                    break\n",
    "            else:\n",
    "                ant.set_emotion()\n",
    "                Q = ant.rule.quality()\n",
    "\n",
    "                mask = np.zeros_like(H)\n",
    "                mask[list(ant.rule.terms.values()), list(ant.rule.terms)] = 1\n",
    "                \n",
    "                self.deposit_pheromones(np.multiply(self.pheromone_map, mask) * Q)\n",
    "                self.evaporate_pheromones()\n",
    "\n",
    "                if a%20 == 0:\n",
    "                    self.plot_pheromones(a)\n",
    "\n",
    "                prev_rules.append(ant.rule)\n",
    "                prev_qualities.append(Q)\n",
    "                \n",
    "            if dups > max_dups:\n",
    "                break\n",
    "\n",
    "        imax = np.argmax(prev_qualities)\n",
    "        \n",
    "        return prev_rules[imax]\n",
    "    \n",
    "    def discover_rules(self):\n",
    "        while(len(self.train_set) > self.max_uncovered):\n",
    "            best_rule = self.dispatch_ants()\n",
    "            corrects = best_rule.correct_set()\n",
    "            self.train_set = pd.concat([self.train_set, corrects, corrects]).drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ee5b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "world = World()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc7fef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "world.discover_rules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1e75db",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('rules.npy', world.discovered_rule_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41251ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load('rules.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e9671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_corrects = 0\n",
    "n_matches = 0\n",
    "for rule in world.discovered_rule_list:\n",
    "    n_corrects += len(rule.correct_set())\n",
    "    n_matches += len(rule.match_set)\n",
    "    \n",
    "n_corrects, n_matches, 100*n_corrects/n_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05968ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f24d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(row):\n",
    "    matching_rules = []\n",
    "    instance = dict(row)\n",
    "    \n",
    "    preddict = defaultdict(lambda: 0)\n",
    "    for rule in world.discovered_rule_list:\n",
    "        if rule.match(instance):\n",
    "            preddict[rule.emotion] += 1\n",
    "            \n",
    "    if preddict:\n",
    "        return max(preddict, key=preddict.get)\n",
    "    return 'unable to predict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b785365",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc2 = discreet.copy(deep=True)\n",
    "disc2.columns = range(len(discreet.columns))\n",
    "disc2.rename(columns={652: 'class'}, inplace=True)\n",
    "#disc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d84e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc2['preds'] = 'nothing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255b3a1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "disc2.at[1143, 'preds'] = predict(disc2.drop(['preds', 'class'], axis=1).iloc[1143])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbb427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc2['preds'][1143]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90943d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in enumerate(disc2.iterrows()):\n",
    "    disc2.at[i, 'preds'] = predict(disc2.drop(['preds', 'class'], axis=1).iloc[i])\n",
    "#disc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2cacc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = disc2.loc[disc2['class'] == disc2['preds']]\n",
    "predicted = disc2.loc[disc2['preds'] != 'unable to predict']\n",
    "len(correct), len(predicted), len(correct)/len(predicted), len(correct)/len(disc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2705d77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc2['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85917fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3ff8d0",
   "metadata": {},
   "source": [
    "# From here it's just random testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04717956",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.unique(np.random.choice(4, 1000, p=[0, 0.33, 0.33, 0.34]), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40a7c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = len(features) - 0\n",
    "Eta = logK - H\n",
    "denom = x * np.sum(Eta, axis=0)\n",
    "Eta = np.divide(Eta, denom)\n",
    "\n",
    "T = np.random.rand(H.shape[0], H.shape[1])\n",
    "Probs = np.multiply(Eta, T)\n",
    "denom = x * np.sum(Probs, axis=0)\n",
    "Probs = np.divide(Probs, denom)\n",
    "np.sum(Probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167f61bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened = Probs.flatten()\n",
    "num = np.random.choice(len(flattened), 1, p=flattened)\n",
    "col = num % 652\n",
    "row = num // 652\n",
    "int(col), int(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba992417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
