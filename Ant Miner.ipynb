{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f3c9b2f",
   "metadata": {},
   "source": [
    "# An implementation of the Ant miner paper for Emotion Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87691e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c2f993",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Resources/Ravdess Dataset.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5195bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_candidate_thresholds = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aeffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_thresholds = df.quantile(np.linspace(0, 1, n_candidate_thresholds))\n",
    "candidate_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb42ec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = df['class'].unique()\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d752a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceaddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "GE_df = pd.DataFrame(columns=cols)\n",
    "GE_df = GE_df.drop('class', axis=1)\n",
    "LT_df = GE_df.copy(deep=True)\n",
    "GE_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d14bf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "GE = np.zeros((n_candidate_thresholds, len(cols) - 1, len(emotions)))\n",
    "LT = GE.copy()\n",
    "GE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27012ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e, emotion in enumerate(emotions):\n",
    "    for i, (quantile, row) in enumerate(candidate_thresholds.iterrows()):\n",
    "        GE_df.loc[i] = df.loc[df['class'] == emotion].drop('class', axis=1).ge(candidate_thresholds.iloc[i]).sum()\n",
    "        LT_df.loc[i] = df.loc[df['class'] == emotion].drop('class', axis=1).lt(candidate_thresholds.iloc[i]).sum()\n",
    "    GE[:, :, e] = GE_df.to_numpy()\n",
    "    LT[:, :, e] = LT_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6019853",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = np.split(df.sample(frac=1), [int(0.75*len(df))])\n",
    "train_len = len(train)\n",
    "test_len = len(test)\n",
    "total_len = train_len + test_len\n",
    "test_len, train_len, total_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d599cddb",
   "metadata": {},
   "source": [
    "# Entropy values\n",
    "H will be a matrix of size (1 x features). Computed in a vectorized way to save time (This would take like 10 mins if it was in a for loop, now it takes 10s). <br>\n",
    "Ok fine I'm looping through the 6 emotions so techhhhnically it's not fully vectorized. Anyway we're computing H only once so I took lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a6bc2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sums_GE = np.sum(GE, axis=2)\n",
    "sums_LT = np.sum(LT, axis=2)\n",
    "#sums_GE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd37cbe4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "repeated_sums_GE = np.repeat(sums_GE[:, :, np.newaxis], GE.shape[2], axis=2)\n",
    "repeated_sums_LT = np.repeat(sums_LT[:, :, np.newaxis], LT.shape[2], axis=2)\n",
    "#repeated_sums_GE[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7885e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_GE = np.divide(GE, repeated_sums_GE, out=np.zeros_like(GE), where=repeated_sums_GE!=0)\n",
    "P_LT = np.divide(LT, repeated_sums_LT, out=np.zeros_like(LT), where=repeated_sums_LT!=0)\n",
    "#P_GE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ee484",
   "metadata": {},
   "outputs": [],
   "source": [
    "logP_GE = np.log2(P_GE, out=np.zeros_like(P_GE), where=P_GE>0)\n",
    "logP_LT = np.log2(P_LT, out=np.zeros_like(P_LT), where=P_LT>0)\n",
    "#logP_GE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f9d06d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GE = -np.sum(np.multiply(P_GE, logP_GE), axis=2)\n",
    "LT = -np.sum(np.multiply(P_LT, logP_LT), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df47614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epv = sums_LT * LT / (sums_GE + sums_LT) + sums_GE * GE / (sums_GE + sums_LT)\n",
    "epv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29d72c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies = np.minimum(\n",
    "                GE[np.argmin(epv, axis=0), np.array(range(len(cols) - 1))],\n",
    "                LT[np.argmin(epv, axis=0), np.array(range(len(cols) - 1))]\n",
    "            )\n",
    "#entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68e13c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = LT[:, :-2]\n",
    "vis = vis.reshape(65,100)\n",
    "plt.imshow(vis)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54108ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = GE[:, :-2]\n",
    "vis = vis.reshape(65,100)\n",
    "plt.imshow(vis)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448814c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = entropies[:-2]\n",
    "vis = vis.reshape(65,10)\n",
    "plt.imshow(vis)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d362d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "def calc_threshold_and_operator(dataset):\n",
    "    GE_df = pd.DataFrame(columns=[dataset.columns[0]])\n",
    "    LT_df = GE_df.copy(deep=True)\n",
    "    GE = np.zeros((n_candidate_thresholds, 1, len(emotions)))\n",
    "    LT = GE.copy()\n",
    "    cands = candidate_thresholds[dataset.columns[0]]\n",
    "    \n",
    "    for e, emotion in enumerate(emotions):\n",
    "        for i, (quantile, row) in enumerate(candidate_thresholds.iterrows()):\n",
    "            GE_df.loc[i] = dataset.loc[dataset['class'] == emotion].drop('class', axis=1).ge(cands.iloc[i]).sum()\n",
    "            LT_df.loc[i] = dataset.loc[dataset['class'] == emotion].drop('class', axis=1).lt(cands.iloc[i]).sum()\n",
    "        GE[:, :, e] = GE_df.to_numpy()\n",
    "        LT[:, :, e] = LT_df.to_numpy()\n",
    "        \n",
    "    sums_GE = np.sum(GE, axis=2)\n",
    "    sums_LT = np.sum(LT, axis=2)\n",
    "    \n",
    "    repeated_sums_GE = np.repeat(sums_GE[:, :, np.newaxis], GE.shape[2], axis=2)\n",
    "    repeated_sums_LT = np.repeat(sums_LT[:, :, np.newaxis], LT.shape[2], axis=2)\n",
    "    \n",
    "    P_GE = np.divide(GE, repeated_sums_GE, out=np.zeros_like(GE), where=repeated_sums_GE!=0)\n",
    "    P_LT = np.divide(LT, repeated_sums_LT, out=np.zeros_like(LT), where=repeated_sums_LT!=0)\n",
    "    \n",
    "    logP_GE = np.log2(P_GE, out=np.zeros_like(P_GE), where=P_GE>0)\n",
    "    logP_LT = np.log2(P_LT, out=np.zeros_like(P_LT), where=P_LT>0)\n",
    "    \n",
    "    GE = -np.sum(np.multiply(P_GE, logP_GE), axis=2)\n",
    "    LT = -np.sum(np.multiply(P_LT, logP_LT), axis=2)\n",
    "    \n",
    "    epv = sums_LT * LT / (sums_GE + sums_LT) + sums_GE * GE / (sums_GE + sums_LT)\n",
    "    vbest = np.argmin(epv, axis=0)\n",
    "    \n",
    "    if GE[vbest, 0] < LT[vbest, 0]:\n",
    "        return float(cands.iloc[vbest]), '>='\n",
    "    \n",
    "    return float(cands.iloc[vbest]), '<'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62298a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop('class', axis=1).columns\n",
    "logK = np.log2(len(emotions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bf303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rule:\n",
    "    def __init__(self, train_set):\n",
    "        self.terms = {}\n",
    "        #{1:1, 18:1}\n",
    "        #{1:('>=', 13), 2:('<', 0.2)}\n",
    "        self.emotion = None\n",
    "        self.numerosity = 0\n",
    "        self.train_set = train_set\n",
    "        self.match_set = train_set\n",
    "    \n",
    "#     def match(self, instance):\n",
    "#         for key, value in instance.items():\n",
    "#             if key in self.terms and self.terms[key] != value:\n",
    "#                 return False\n",
    "#         return True\n",
    "    \n",
    "#     def match_set(self):\n",
    "#         gdic = {'f' + str(key): val[1] for key, val in terms.items() if val[0] == '>'}\n",
    "#         ldic = {'f' + str(key): val[1] for key, val in terms.items() if val[0] == '<'}\n",
    "#         g = train_set.loc[np.all(train_set[list(gdic)] > pd.Series(gdic), axis=1)]\n",
    "#         return g.loc[np.all(g[list(ldic)] <= pd.Series(ldic), axis=1)]\n",
    "    \n",
    "    def new_match_set(self, term):\n",
    "        if term['value'][0] == '>=':\n",
    "            return self.match_set.loc[self.match_set['f' + str(term['feature'])] >= term['value'][1]]\n",
    "        return self.match_set.loc[self.match_set['f' + str(term['feature'])] < term['value'][1]]\n",
    "    \n",
    "    def unmatch_set(self):\n",
    "        return pd.concat([self.train_set, self.match_set, self.match_set]).drop_duplicates(keep=False)\n",
    "    \n",
    "    def correct_set(self):\n",
    "        return self.match_set.loc[self.match_set['class'] == self.emotion]\n",
    "    \n",
    "    def exists(self, feature):\n",
    "        return feature in self.terms\n",
    "        \n",
    "    def addTerm(self, term, new_matches):\n",
    "        self.terms[term['feature']] = term['value']\n",
    "        self.match_set = new_matches\n",
    "        self.numerosity += 1\n",
    "        \n",
    "    def computeTerm(self, attribute):\n",
    "        pass\n",
    "        \n",
    "    def quality(self):\n",
    "        matches = self.match_set\n",
    "        unmatches = self.unmatch_set()\n",
    "        TP = len(matches.loc[matches['class'] == self.emotion])\n",
    "        FP = len(matches.loc[matches['class'] != self.emotion])\n",
    "        FN = len(unmatches.loc[unmatches['class'] == self.emotion])\n",
    "        TN = len(unmatches.loc[unmatches['class'] != self.emotion])\n",
    "        \n",
    "        sens = TP / (TP + FN)\n",
    "        spec = TN / (FP + TN)\n",
    "        \n",
    "        return sens * spec\n",
    "        \n",
    "    def prune(self):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e88ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_probs(probs):\n",
    "    return probs / np.sum(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796acd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ant:\n",
    "    def __init__(self, index, train_set, n_tries, min_coverage):\n",
    "        self.index = index\n",
    "        self.rule = Rule(train_set)\n",
    "        self.n_tries = n_tries\n",
    "        self.min_coverage = min_coverage\n",
    "    \n",
    "    def traverse(self, T):\n",
    "        Eta = logK - entropies\n",
    "\n",
    "        Probs = np.multiply(Eta, T)\n",
    "        Probs = normalize_probs(Probs)\n",
    "        \n",
    "        should_continue = True\n",
    "        while(should_continue):\n",
    "            tries = self.n_tries\n",
    "            while(tries):\n",
    "                chosen = int(np.random.choice(len(Probs), 1, p=Probs))\n",
    "                v, op = calc_threshold_and_operator(self.rule.match_set[['f' + str(chosen), 'class']])\n",
    "                term = {'feature': chosen, 'value': (op, v)}\n",
    "                new_cov = self.rule.new_match_set(term)\n",
    "                if len(new_cov) >= self.min_coverage:\n",
    "                    self.rule.addTerm(term, new_cov)\n",
    "                    print(\"Added \" + str(term))\n",
    "                    \n",
    "                    Probs[term['feature']] = 0\n",
    "                    Probs = normalize_probs(Probs)\n",
    "                    \n",
    "                    should_continue = True\n",
    "                    break\n",
    "                tries -= 1\n",
    "                should_continue = False\n",
    "                \n",
    "    def set_emotion(self):\n",
    "        matches = self.rule.match_set\n",
    "        self.rule.emotion = matches['class'].mode()[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439d4fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class World:\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_coverage = 10,\n",
    "        max_uncovered = 10,\n",
    "        n_ants = 500,\n",
    "        max_dups = 1,\n",
    "        chances_before_brutally_murdering_ant = 10,\n",
    "        train_set = train.copy(deep=True),\n",
    "    ):\n",
    "    \n",
    "        self.min_coverage = min_coverage\n",
    "        self.max_uncovered = max_uncovered\n",
    "        self.n_ants = n_ants\n",
    "        self.max_dups = max_dups\n",
    "        self.chances_before_brutally_murdering_ant = chances_before_brutally_murdering_ant\n",
    "        self.train_set = train_set\n",
    "        self.pheromone_map = np.ones_like(entropies) * (1 / len(features))\n",
    "        self.discovered_rule_list = []\n",
    "\n",
    "    def deposit_pheromones(self, new_pheromones):\n",
    "        self.pheromone_map = self.pheromone_map + new_pheromones\n",
    "        \n",
    "    def evaporate_pheromones(self):\n",
    "        self.pheromone_map = self.pheromone_map / np.sum(self.pheromone_map)\n",
    "        \n",
    "    def plot_pheromones(self, a):\n",
    "        vis = self.pheromone_map[:-1]\n",
    "        vis = vis.reshape(31,21)\n",
    "        plt.imshow(vis)\n",
    "        plt.colorbar()\n",
    "        plt.savefig('Plots/Generation' + str(len(self.discovered_rule_list)) + 'ant' + str(a))\n",
    "        plt.clf()\n",
    "    \n",
    "    def dispatch_ants(self):\n",
    "        prev_rules = []\n",
    "        prev_qualities = []\n",
    "        dups = 0\n",
    "        for a in range(self.n_ants):\n",
    "            ant = Ant(a, self.train_set, self.chances_before_brutally_murdering_ant, self.min_coverage)\n",
    "            print(\"initialized ant \" + str(a))\n",
    "            ant.traverse(self.pheromone_map)\n",
    "            \n",
    "            for prev_rule in prev_rules:\n",
    "                if ant.rule == prev_rule:\n",
    "                    dups += 1\n",
    "                    break\n",
    "            else:\n",
    "                ant.set_emotion()\n",
    "                Q = ant.rule.quality()\n",
    "\n",
    "                mask = np.zeros_like(entropies)\n",
    "                mask[list(ant.rule.terms)] = 1\n",
    "                \n",
    "                self.deposit_pheromones(np.multiply(self.pheromone_map, mask) * Q)\n",
    "                self.evaporate_pheromones()\n",
    "\n",
    "                if a%20 == 0:\n",
    "                    self.plot_pheromones(a)\n",
    "\n",
    "                prev_rules.append(ant.rule)\n",
    "                prev_qualities.append(Q)\n",
    "                \n",
    "            if dups > self.max_dups:\n",
    "                break\n",
    "\n",
    "        imax = np.argmax(prev_qualities)\n",
    "        \n",
    "        return prev_rules[imax]\n",
    "    \n",
    "    def discover_rules(self):\n",
    "        while(len(self.train_set) > self.max_uncovered):\n",
    "            best_rule = self.dispatch_ants()\n",
    "            corrects = best_rule.correct_set()\n",
    "            self.train_set = pd.concat([self.train_set, corrects, corrects]).drop_duplicates(keep=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8964ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "world = World(n_ants=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3fb08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "world.discover_rules()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3ff8d0",
   "metadata": {},
   "source": [
    "# From here it's just random testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04717956",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.unique(np.random.choice(4, 1000, p=[0, 0.33, 0.33, 0.34]), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40a7c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = len(features) - 0\n",
    "Eta = logK - H\n",
    "denom = x * np.sum(Eta, axis=0)\n",
    "Eta = np.divide(Eta, denom)\n",
    "\n",
    "T = np.random.rand(H.shape[0], H.shape[1])\n",
    "Probs = np.multiply(Eta, T)\n",
    "denom = x * np.sum(Probs, axis=0)\n",
    "Probs = np.divide(Probs, denom)\n",
    "np.sum(Probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167f61bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened = Probs.flatten()\n",
    "num = np.random.choice(len(flattened), 1, p=flattened)\n",
    "col = num % 652\n",
    "row = num // 652\n",
    "int(col), int(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba992417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
